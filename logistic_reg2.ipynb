{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXr1IptletjbEEdfhwLVty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q-1)Question 1: What is Logistic Regression, and how\n","does it differ from Linear Regression?\n","\n","Ans) Logistic Regression is a supervised machine\n","learning algorithm used for classification problems.\n","\n","\n","\n","It predicts the probability of a categorical dependent\n","variable (usually binary: Yes/No, 0/1, Spam/Not Spam).\n","\n","\n","\n","Instead of fitting a straight line, it uses a logistic\n","(sigmoid) function to map predicted values into the\n","range 0 to 1.\n","\n","\n","\n","The output represents the probability of belonging to a\n","particular class.\n","\n","\n","\n","Decision boundary is made using a threshold\n","(commonly 0.5):\n","● If probability ≥ 0.5 → class 1\n","● If probability < 0.5 → class 0\n","\n","Mathematical Form:\n","p(y=1/x)=1/1+e^-(c+mx1)\n","\n","Linear Regression\n","● Linear Regression is used for regression problems,\n","i.e., predicting continuous numeric values (e.g.,\n","salary prediction, house price prediction).\n","\n","● It assumes a linear relationship between\n","independent variables (X) and the dependent variable\n","(Y).\n","\n","● The output is not restricted, so predictions can be any\n","real value (−∞ to +∞).\n","\n","Mathematical Form:\n","y=c+m1x1+m2x2+.....+m(n)x(n)"],"metadata":{"id":"KGVtO00NGq20"}},{"cell_type":"markdown","source":["Question 2: Explain the role of the Sigmoid function in\n","Logistic Regression.  \n","\n","ans)Role of the Sigmoid Function in Logistic Regression\n","The sigmoid function (also called the logistic function)\n","plays a central role in Logistic Regression because it\n","converts the linear output of the regression equation into\n","a probability value between 0 and 1."],"metadata":{"id":"1SNT1FrFHZd5"}},{"cell_type":"markdown","source":["Question 3: What is Regularization in Logistic Regression\n","and why is it needed?\n","\n","ans)Regularization is a technique used in logistic\n","regression (and other ML models) to prevent overfitting\n","by adding a penalty term to the cost function.\n","\n","\n","It controls the complexity of the model by discouraging\n","excessively large weights (coefficients).\n","\n","it is Needed because:-\n","1. Prevents Overfitting\n","2. Controls Coefficient Size\n","3. Improves Generalization\n","\n","\n","Question 4: What are some common evaluation metrics\n","for classification models, and why are they important?\n","\n","\n","ans)Classification models (like Logistic Regression,\n","Decision Trees, etc.) are evaluated using metrics that\n","compare predicted vs actual outcomes.\n","\n","These metrics\n","are important because they measure accuracy, reliability,\n","and usefulness of the model\n","\n","\n","1. Accuracy\n","● Definition: Ratio of correctly predicted instances to\n","total instances.\n","accuracy=TP+TN/TP+TN+FP+FN\n","\n","\n","2. Precision\n","● Definition: Out of all predicted positives, how many\n","are actually positive.\n","precision=TP/TP+FP\n","\n","\n","3. Recall (Sensitivity / True Positive Rate)\n","● Definition: Out of all actual positives, how many did\n","the model correctly identify.\n","recall=TP/TP+FN\n","\n","\n","4. F1-Score\n","● Definition: Harmonic mean of Precision and Recall.\n","F1=2*precision*recall/precision+recall\n","\n","\n","5. ROC Curve & AUC (Area Under Curve)\n","● ROC Curve: Plots True Positive Rate vs False\n","Positive Rate.\n","\n","● AUC: Measures the overall ability of the model to\n","distinguish between classes.\n","\n","\n","● Importance: Good for comparing models across\n","different thresholds.\n","\n","\n","6. Confusion Matrix\n","● A table showing counts of TP, TN, FP, FN.\n","\n"],"metadata":{"id":"RblhsY5kHlLS"}},{"cell_type":"code","source":["#Question 5: Write a Python program that loads a CSV file\n","#into a Pandas DataFrame,\n","#splits into train/test sets, trains a Logistic Regression\n","#model, and prints its accuracy.\n","\n","# Import necessary libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset from sklearn\n","data = load_breast_cancer()\n","\n","# Convert dataset into a Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","print(\"First 5 rows of dataset:\")\n","print(df.head())\n","\n","# Split features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split dataset into train/test sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Initialize Logistic Regression model\n","model = LogisticRegression(max_iter=10000)\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\nLogistic Regression Accuracy: {accuracy:.4f}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKQz-aBqJBSP","executionInfo":{"status":"ok","timestamp":1756916348961,"user_tz":-330,"elapsed":3492,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"b88bb118-ff67-481b-d14d-9900330c5ce3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of dataset:\n","   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n","0        17.99         10.38          122.80     1001.0          0.11840   \n","1        20.57         17.77          132.90     1326.0          0.08474   \n","2        19.69         21.25          130.00     1203.0          0.10960   \n","3        11.42         20.38           77.58      386.1          0.14250   \n","4        20.29         14.34          135.10     1297.0          0.10030   \n","\n","   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n","0           0.27760          0.3001              0.14710         0.2419   \n","1           0.07864          0.0869              0.07017         0.1812   \n","2           0.15990          0.1974              0.12790         0.2069   \n","3           0.28390          0.2414              0.10520         0.2597   \n","4           0.13280          0.1980              0.10430         0.1809   \n","\n","   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n","0                 0.07871  ...          17.33           184.60      2019.0   \n","1                 0.05667  ...          23.41           158.80      1956.0   \n","2                 0.05999  ...          25.53           152.50      1709.0   \n","3                 0.09744  ...          26.50            98.87       567.7   \n","4                 0.05883  ...          16.67           152.20      1575.0   \n","\n","   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n","0            0.1622             0.6656           0.7119                0.2654   \n","1            0.1238             0.1866           0.2416                0.1860   \n","2            0.1444             0.4245           0.4504                0.2430   \n","3            0.2098             0.8663           0.6869                0.2575   \n","4            0.1374             0.2050           0.4000                0.1625   \n","\n","   worst symmetry  worst fractal dimension  target  \n","0          0.4601                  0.11890       0  \n","1          0.2750                  0.08902       0  \n","2          0.3613                  0.08758       0  \n","3          0.6638                  0.17300       0  \n","4          0.2364                  0.07678       0  \n","\n","[5 rows x 31 columns]\n","\n","Logistic Regression Accuracy: 0.9649\n"]}]},{"cell_type":"code","source":["#Question 6: Write a Python program to train a Logistic Regression model using L2\n","#regularization (Ridge) and print the model coefficients and accuracy.\n","#(Use Dataset from sklearn package)\n","\n","# Import necessary libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset from sklearn\n","data = load_breast_cancer()\n","\n","# Convert dataset into Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Split features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Initialize Logistic Regression model with L2 regularization (default)\n","model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Print model coefficients and accuracy\n","print(\"Model Coefficients:\")\n","print(model.coef_)\n","\n","print(\"\\nIntercept:\")\n","print(model.intercept_)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\nLogistic Regression (L2) Accuracy: {accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uI_yFZ62YDYj","executionInfo":{"status":"ok","timestamp":1756916471118,"user_tz":-330,"elapsed":1374,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"5a9b5650-1770-4883-a8d9-386b0e9da8ad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Coefficients:\n","[[ 0.80708196  0.11328911 -0.28307687  0.02521483 -0.16733751 -0.20217576\n","  -0.45506264 -0.25243782 -0.30921281 -0.03116865 -0.05511739  1.10334223\n","   0.08564572 -0.09595851 -0.02231761  0.05911651 -0.02139347 -0.03540418\n","  -0.04039299  0.0137089   0.09521451 -0.37693116 -0.08781235 -0.01459524\n","  -0.32483321 -0.74767161 -1.32332634 -0.56343008 -0.78785848 -0.09156122]]\n","\n","Intercept:\n","[29.17330007]\n","\n","Logistic Regression (L2) Accuracy: 0.9649\n"]}]},{"cell_type":"code","source":["#Question 7: Write a Python program to train a Logistic Regression model for multiclass\n","#classification using multi_class='ovr' and print the classification report.\n","#(Use Dataset from sklearn package)\n","\n","# Import necessary libraries\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Load Iris dataset (multiclass classification problem)\n","data = load_iris()\n","\n","# Convert dataset into Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","print(\"First 5 rows of dataset:\")\n","print(df.head())\n","\n","# Split features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Train/Test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Initialize Logistic Regression with One-vs-Rest (OvR)\n","model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=10000)\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Print classification report\n","print(\"\\nClassification Report (One-vs-Rest):\")\n","print(classification_report(y_test, y_pred, target_names=data.target_names))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUVaI7pRYeFe","executionInfo":{"status":"ok","timestamp":1756916546298,"user_tz":-330,"elapsed":73,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"99b221d0-a48a-4b21-d4dc-7c9a3cef1d71"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of dataset:\n","   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n","0                5.1               3.5                1.4               0.2   \n","1                4.9               3.0                1.4               0.2   \n","2                4.7               3.2                1.3               0.2   \n","3                4.6               3.1                1.5               0.2   \n","4                5.0               3.6                1.4               0.2   \n","\n","   target  \n","0       0  \n","1       0  \n","2       0  \n","3       0  \n","4       0  \n","\n","Classification Report (One-vs-Rest):\n","              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        10\n","  versicolor       1.00      0.80      0.89        10\n","   virginica       0.83      1.00      0.91        10\n","\n","    accuracy                           0.93        30\n","   macro avg       0.94      0.93      0.93        30\n","weighted avg       0.94      0.93      0.93        30\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n","#hyperparameters for Logistic Regression and print the best parameters and validation\n","#accuracy\n","\n","# Import necessary libraries\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","# Load Iris dataset (multiclass classification)\n","data = load_iris()\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Split features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Train/Test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Define Logistic Regression model\n","log_reg = LogisticRegression(max_iter=10000)\n","\n","# Define parameter grid for GridSearchCV\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10, 100],   # Regularization strength\n","    'penalty': ['l1', 'l2'],        # Penalty type\n","    'solver': ['liblinear']         # 'liblinear' supports both l1 and l2\n","}\n","\n","# Initialize GridSearchCV\n","grid = GridSearchCV(\n","    estimator=log_reg,\n","    param_grid=param_grid,\n","    cv=5,              # 5-fold cross-validation\n","    scoring='accuracy',\n","    n_jobs=-1\n",")\n","\n","# Fit GridSearchCV\n","grid.fit(X_train, y_train)\n","\n","# Print best parameters and validation accuracy\n","print(\"Best Parameters:\", grid.best_params_)\n","print(\"Best Cross-Validation Accuracy: {:.4f}\".format(grid.best_score_))\n","\n","# Evaluate on test set\n","test_accuracy = grid.score(X_test, y_test)\n","print(\"Test Set Accuracy: {:.4f}\".format(test_accuracy))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5zQc39lYwls","executionInfo":{"status":"ok","timestamp":1756916653706,"user_tz":-330,"elapsed":2783,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"f1c72fcf-13e0-4053-c2a5-7934846f6b76"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n","Best Cross-Validation Accuracy: 0.9667\n","Test Set Accuracy: 0.9667\n"]}]},{"cell_type":"code","source":["#Question 9: Write a Python program to standardize the features before training Logistic\n","#Regression and compare the model's accuracy with and without scaling.\n","#(Use Dataset from sklearn package)\n","\n","# Import libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset (Breast Cancer dataset from sklearn)\n","data = load_breast_cancer()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Train/Test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# --------------------------\n","# Logistic Regression WITHOUT scaling\n","# --------------------------\n","model_no_scaling = LogisticRegression(max_iter=10000)\n","model_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = model_no_scaling.predict(X_test)\n","acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","# --------------------------\n","# Logistic Regression WITH scaling\n","# --------------------------\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","model_with_scaling = LogisticRegression(max_iter=10000)\n","model_with_scaling.fit(X_train_scaled, y_train)\n","y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n","acc_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n","\n","\n","print(f\"Accuracy WITHOUT Scaling: {acc_no_scaling:.4f}\")\n","print(f\"Accuracy WITH Scaling:    {acc_with_scaling:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARegqYOaZK6M","executionInfo":{"status":"ok","timestamp":1756916755179,"user_tz":-330,"elapsed":852,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"6147ea8f-b035-42f4-d1f6-966f85689a31"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy WITHOUT Scaling: 0.9649\n","Accuracy WITH Scaling:    0.9825\n"]}]},{"cell_type":"markdown","source":["Question 10: Imagine you are working at an e-commerce company that wants to\n","predict which customers will respond to a marketing campaign. Given an imbalanced\n","dataset (only 5% of customers respond), describe the approach you’d take to build a\n","Logistic Regression model — including data handling, feature scaling, balancing\n","classes, hyperparameter tuning, and evaluating the model for this real-world business\n","use case.\n","\n","\n","ans)\n","Approach for Building Logistic Regression Model in an Imbalanced Dataset (E-commerce Marketing Campaign)\n","1. Understanding the Problem\n","\n","Goal: Predict which customers will respond (positive class) to a marketing campaign.\n","\n","Dataset: Highly imbalanced (only ~5% positive class).\n","\n","Risk: Model might predict “No Response” for everyone and still get 95% accuracy → so accuracy is not the right metric here.\n","2. Data Handling\n","\n","Collect and preprocess data:\n","\n","Handle missing values, duplicates, and outliers.\n","\n","Encode categorical variables (e.g., one-hot encoding).\n","\n","Standardize numerical features (Logistic Regression assumes feature scale matters).\n","\n","Feature selection/engineering:\n","\n","Create customer behavior features (purchase frequency, last purchase, average spend).\n","\n","Drop irrelevant/noisy features.\n","\n","3. Feature Scaling\n","\n","Logistic Regression is sensitive to feature magnitude.\n","\n","Use StandardScaler (mean=0, variance=1) or MinMaxScaler to bring all features to the same scale.\n","\n","4. Handling Class Imbalance\n","\n","Since only 5% are responders:\n","\n","Option 1: Resampling\n","\n","Oversampling minority class (e.g., SMOTE) to increase responders.\n","\n","Undersampling majority class (but risk of losing information).\n","\n","Option 2: Class Weights\n","\n","Use class_weight='balanced' in Logistic Regression → automatically penalizes misclassifying minority class.\n","\n","In practice: Try both oversampling + class weights.\n","\n","5. Model Training with Logistic Regression\n","\n","Initialize Logistic Regression:\n","\n","LogisticRegression(class_weight='balanced', max_iter=10000)\n","\n","6. Hyperparameter Tuning\n","\n","Use GridSearchCV or RandomizedSearchCV to tune:\n","\n","C (regularization strength).\n","\n","Penalty (l1 or l2).\n","\n","Solver (liblinear or saga for L1/L2).\n","Example grid:\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","    'penalty': ['l1', 'l2'],\n","    'solver': ['liblinear', 'saga']\n","}\n","\n","7. Evaluation Metrics\n","\n","Accuracy is misleading for imbalanced datasets.\n","Instead, use:\n","\n","Precision (how many predicted responders are correct).\n","\n","Recall (Sensitivity) (how many actual responders we capture).\n","\n","F1-Score (balance of precision and recall).\n","\n","ROC-AUC (overall model discrimination ability).\n","\n","PR-AUC (Precision-Recall Curve) — especially useful for highly imbalanced data.\n","\n","8. Deployment & Monitoring\n","\n","Deploy model to production to score new customers.\n","\n","Monitor drift (customer behavior changes over time).\n","\n","Retrain model periodically with new data.\n","\n","Evaluate business impact: How many extra sales are generated by targeting predicted responders?"],"metadata":{"id":"KdM0m5S8ZzRv"}}]}